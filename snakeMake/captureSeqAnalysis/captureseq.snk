import os
import subprocess as sp
from glob import glob
import re

shell.executable("/usr/bin/bash")

configfile : "config.yaml"
WKDIR = os.getcwd()

r1_regex = '_R1_'
r2_regex = '_R2_'
fq_R1_names = {}
fq_R2_names = {}
fq_names = glob('fastq-*/*.fastq.gz', recursive=True)
for s in config['samples'].keys():
    for f in glob(f'{config["samples"][s]}*.fastq.gz'):
        if r1_regex in f:
            fq_R1_names[s] = f
        elif r2_regex in f:
            fq_R2_names[s] = f


os.makedirs("logs", exist_ok=True)
os.makedirs("TEMP", exist_ok=True)
os.makedirs("final", exist_ok=True)

def getAssemblyName(v):
    return os.path.basename(v).replace('.fa', '')

SAMPLES = config["samples"]

wildcard_constraints:
    sample = "|".join(SAMPLES)

localrules: consolidate_stats, plots_final

onerror:
    print("Error! Mailing log...")
    shell("tail -n 100 {log} | mail -s 'CaptureSeq' derek.bickhart@hendrix-genetics.com")
    print("Done")

rule all:
    input:
        expand("linkage/{sample}/{sample}.clusters.tab", sample=SAMPLES),
        expand("contaminant/{sample}/contaminants.tab", sample=SAMPLES),
        expand("mapped/{sample}.depth", sample=SAMPLES), 
        "final/consolidated_table.tab", 
        "final/combined_bam_stats.pdf", 
        'final/PLOTSCOPIED'

rule bam_stats:
    input:
        "final/combined_bam_stats.pdf"

rule align_vector:
    input:
        fq1 = lambda wildcards: fq_R1_names[wildcards.sample],
        fq2 = lambda wildcards: fq_R2_names[wildcards.sample],
        ref = config["vector"]
    output:
        bam = "mapped/{sample}.vectormap.bam",
        bai = "mapped/{sample}.vectormap.bam.bai"
    threads: 10
    params:
        sname = "{sample}"
    log:
        "logs/{sample}/vectormap.log"
    resources:
        mem_mb=25000,
        runtime="1d",
        threads=10,
        outstr=lambda wildcards: f'logs/vectormap/{wildcards.sample}'
    shell:
        """
        minimap2 -t {resources.threads} -ax sr \
            -R '@RG\\tID:{params.sname}\\tSM:{params.sname}' \
            {input.ref} \
            {input.fq1} \
            {input.fq2} \
            | samtools sort -T {params.sname}.temp -o {output.bam} - 2> {log}
        samtools index {output.bam}
        """

rule extract_reads:
    input:
        "mapped/{sample}.vectormap.bam"
    output:
        mUnmap1 = temp("rawreads/{sample}/{sample}_mateUnmapped_R1.fq"),
        mUnmap2 = temp("rawreads/{sample}/{sample}_mateUnmapped_R2.fq"),
        mMap1 = temp("rawreads/{sample}/{sample}_R1_mateMapped.fq"),
        mMap2 = temp("rawreads/{sample}/{sample}_R2_mateMapped.fq"),
        links = temp("rawreads/{sample}/{sample}_links.sam")
    log: config['logdir'] + "/{sample}/extract.log"
    resources:
        mem_mb=5000,
        runtime="1d",
        threads=1,
        outstr=lambda wildcards: f'logs/extract_reads/{wildcards.sample}'
    shell:
        """
        samtools fastq -f 12 {input} -1 {output.mUnmap1} -2 {output.mUnmap2}
        samtools fastq -f 68 -F 8 {input} > {output.mMap1}
        samtools fastq -f 132 -F 8 {input} > {output.mMap2}
        samtools view -f 8 -F 4 {input} > {output.links}
        """

rule cat_unmap:
    input:
        mUnmap1 = "rawreads/{sample}/{sample}_R1_mateMapped.fq",
        mUnmap2 = "rawreads/{sample}/{sample}_R2_mateMapped.fq"
    output:
        catunmap = temp("filtered/{sample}/{sample}.cat.unmapped.fq")
    resources:
        mem_mb = 1000,
        runtime = "1h",
        threads = 1,
        outstr=lambda wildcards: f'logs/cat/{wildcards.sample}'
    shell:
        """
        cat {input.mUnmap1} {input.mUnmap2} > {output.catunmap}
        """

rule remap_unmapped:
    input:
        ref = config['reference'],
        mUnmap1 = "rawreads/{sample}/{sample}_R1_mateMapped.fq",
        mUnmap2 = "rawreads/{sample}/{sample}_R2_mateMapped.fq",
        catunmap = "filtered/{sample}/{sample}.cat.unmapped.fq"
    log: config['logdir'] + "/{sample}/remap_unmap.log"
    params: samp = "{sample}"
    output:
        readalign = "filtered/{sample}/{sample}.readcontig.bam",
        unmaplinks = "linkage/{sample}/{sample}.readlinks.bed"
    resources:
        mem_mb=25000,
        runtime="1d",
        threads=10,
        outstr=lambda wildcards: f'logs/remap/{wildcards.sample}'
    shell:
        """
        minimap2 -t {resources.threads} -ax sr \
            -R '@RG\\tID:{params.samp}\\tSM:{params.samp}' \
            {input.ref} \
            {input.catunmap} \
            | samtools sort -T {params.samp}.utemp -o {output.readalign} - 2> {log}
        echo "Done with alignment" > {log}
        bedtools bamtobed -i {output.readalign} > {output.unmaplinks} 2> {log}
        """


# OUTPUT: read cluster assignments and read evidence for each
rule match_mates:
    input:
        ref = config['vector'],
        links = "rawreads/{sample}/{sample}_links.sam",
        unmaplinks = "linkage/{sample}/{sample}.readlinks.bed"
    log: config['logdir'] + "/{sample}/mate_match.log"
    params:
        samp = "{sample}",
        edge = 1000,
        script = workflow.basedir + "/scripts/tabulateMates.py"
    output:
        clusters = "linkage/{sample}/{sample}.clusters.tab"
    resources:
        mem_mb=10000,
        runtime="1d",
        threads=1,
        outstr=lambda wildcards: f'logs/match_mates/{wildcards.sample}'
    shell:
        """
        python {params.script} {input.ref} {input.links} {input.unmaplinks} {output.clusters}
        """

rule create_calibrator_depth:
    input:
        ref = config['reference'],
        fq1 = lambda wildcards: fq_R1_names[wildcards.sample],
        fq2 = lambda wildcards: fq_R2_names[wildcards.sample]
    log: config['logdir'] + "/{sample}/calibrator_depth.log"
    params:
        samp = "{sample}",
        script = workflow.basedir + "/scripts/depth_estimate.py",
        calibrator = config['calibrator_region']
    output:
        bam = "mapped/{sample}.depthmap.bam", 
        bai = "mapped/{sample}.depthmap.bam.bai",
        depth = "mapped/{sample}.depth"
    resources:
        mem_mb=25000,
        runtime="1d",
        threads=10,
        outstr=lambda wildcards: f'logs/calibrator_depth/{wildcards.sample}'
    shell:
        """
        minimap2 -t {resources.threads} -ax sr \
            -R '@RG\\tID:{params.samp}\\tSM:{params.samp}' \
            {input.ref} \
            {input.fq1} \
            {input.fq2} \
            | samtools sort -T {params.samp}.temp -o {output.bam} - 2> {log}
        samtools index {output.bam}

        python {params.script} {output.bam} {params.calibrator} {output.depth}
        """

rule get_bam_stats:
    input:
        bam = "mapped/{sample}.depthmap.bam",
        bai = "mapped/{sample}.depthmap.bam.bai",
        reference = config["reference"]
    output:
        stats = temp("final/{sample}.stats.tab")
    log:
        config['logdir'] + "/{sample}/get_bam_stats.log"
    resources:
        mem_mb=9000,
        runtime="2h",
        threads=2,
        outstr=lambda wildcards: f'logs/get_bam_stats/{wildcards.sample}'
    params:
        sname = "{sample}",
        threshold = 4
    shell:
        """
        python3 {workflow.basedir}/scripts/calc_bam_stats.py {input.bam} {params.threshold} {params.sname} {output.stats}
        """

rule consolidate_stats:
    input:
        expand("final/{samples}.stats.tab", samples=SAMPLES)
    output:
        "final/combined_bam_stats.tab"
    run:
        with open(output[0], 'w') as outfile:
            outfile.write("SNAME\tThreshBp\tZbp\tMean\tQ25\tMedian\tQ75\tMax\tStdev\tsub15\tsub30\tgt30\n")
            for i in input:
                with open(i, 'r') as infile:
                    for l in infile:
                        outfile.write(l)

rule plot_stats:
    input:
        "final/combined_bam_stats.tab"
    output:
        "final/combined_bam_stats.pdf"
    params:
        samples = 50
    resources:
        mem_mb=15000,
        runtime="2h",
        threads=2,
        outstr=lambda wildcards: f'logs/plot_stats/plot_stats'
    shell:
        """
        python3 {workflow.basedir}/scripts/plot_bam_stats.py {input} {params.samples} {output}
        """

rule mash_screen:
    input:
        library = config['mash_db'],
        fq1 = lambda wildcards: fq_R1_names[wildcards.sample],
        fq2 = lambda wildcards: fq_R2_names[wildcards.sample], 
        depth = "mapped/{sample}.depth"
    log: config['logdir'] + "/{sample}/mash_screen.log"
    params:
        samp = "{sample}",
        script = workflow.basedir + "/scripts/contaminantID.py",
        plasmid_names = config['contaminant_file']
    output:
        distances = "contaminant/{sample}/mash_screen.tab",
        filtered = "contaminant/{sample}/mash_filtered_screen.tab",
        status = "contaminant/{sample}/contaminants.tab"
    resources:
        mem_mb=15000,
        runtime="1d",
        threads=10,
        outstr=lambda wildcards: f'logs/mash_screen/{wildcards.sample}'
    shell:
        """
        mash screen -p 10 -w {input.library} {input.fq1} {input.fq2} > {output.distances}

        python {params.script} {output.distances} {params.plasmid_names} {output.filtered} {output.status} {input.depth}
        """

rule refine_breaks:
    input:
        cluster = "linkage/{sample}/{sample}.clusters.tab",
        depth = "mapped/{sample}.depth", 
        bam = "mapped/{sample}.depthmap.bam"
    log: config['logdir'] + "/{sample}/refine_breaks.log"
    params:
        samp = "{sample}",
        script = workflow.basedir + "/scripts/refineBAMBreakpoints.py",
        basename = "linkage/{sample}/{sample}.refinement"
    output:
        "linkage/{sample}/{sample}.refinement.tab"
    resources:
        mem_mb=10000,
        runtime="1d",
        threads=1,
        outstr=lambda wildcards: f'logs/refinement/{wildcards.sample}'
    shell:
        """
        python {params.script} -f {input.cluster} -b {input.bam} -o {params.basename} -d {input.depth}
        """

rule bedtools_intersect:
    input:
        refinement = "linkage/{sample}/{sample}.refinement.tab"
    log: config['logdir'] + "/{sample}/bedtools_intersect.log"
    params:
        script = workflow.basedir + "/scripts/bedtools_intersect.py",
        genebed = config['wally_annotation']
    output:
        bedfile = temp("linkage/{sample}/{sample}.temp.bed"),
        overlaps = "linkage/{sample}/{sample}.geneoverlaps.tab"
    resources:
        mem_mb=5000,
        runtime="1h",
        threads=1,
        outstr=lambda wildcards: f'logs/bedtools/{wildcards.sample}'
    shell:
        """
        python {params.script} -f {input.refinement} -o {output.overlaps} -b {output.bedfile} -g {params.genebed}
        """
    
rule consolidate_table:
    input:
        depth = expand("mapped/{sample}.depth", sample = SAMPLES),
        status = expand("contaminant/{sample}/contaminants.tab", sample = SAMPLES),
        clusters = expand("linkage/{sample}/{sample}.refinement.tab", sample = SAMPLES),
        genes = expand("linkage/{sample}/{sample}.geneoverlaps.tab", sample=SAMPLES)
    log: config['logdir'] + "/consolidate_table.log"
    params:
        script = workflow.basedir + "/scripts/consolidate.py", 
        samples = " -s ".join(SAMPLES),
        depths = "final/depthfiles.list",
        states = "final/statefiles.list",
        nodes = "final/nodes.list",
        overlaps = "final/genes.list"
        #depths = lambda wildcards, input: " -d ".join(input.depth),
        #states = lambda wildcards, input: " -c ".join(input.status),
        #nodes = lambda wildcards, input: " -f ".join(input.clusters)
    output:
        "final/consolidated_table.tab"
    resources:
        mem_mb=10000,
        runtime="1d",
        threads=1,
        outstr=lambda wildcards: f'logs/consolidate/consolidation'
    shell:
        """
        echo {input.depth} > {params.depths}
        echo {input.status} > {params.states}
        echo {input.clusters} > {params.nodes}
        echo {input.genes} > {params.overlaps}

        python {params.script} -s {params.samples} -d {params.depths} -c {params.states} -f {params.nodes} -o {output} -g {params.overlaps}
        """

rule wally_inputs:
    input:
        refined = "linkage/{sample}/{sample}.refinement.tab",
        depth = "mapped/{sample}.depth"
    log: config['logdir'] + "/{sample}/wally_input.log"
    params:
        script = workflow.basedir + "/scripts/createWallyBed.py",
        samp = "{sample}",
        extension = 1000
    output:
        ifile = "linkage/{sample}.wallyregions.bed"
    resources:
        mem_mb=10000,
        runtime="1d",
        threads=1,
        outstr=lambda wildcards: f'logs/wallyinput/{wildcards.sample}'
    shell: 
        """
        python {params.script} -f {input.refined} -d {input.depth} -s {params.samp} -e {params.extension} -o {output.ifile}
        """



rule wally_snapshots:
    input:
        bam = "mapped/{sample}.depthmap.bam",
        ifile = "linkage/{sample}.wallyregions.bed",
        ref = config['reference']
    log: config['logdir'] + "/{sample}/wally_snapshot.log"
    params:
        x = 2048,
        y = 4096,
        genebed = config['wally_annotation']
    output:
        directory('final/plots/{sample}')
    resources:
        mem_mb=4000,
        runtime="1h",
        threads=1,
        outstr=lambda wildcards: f'logs/wally/{wildcards.sample}'
    shell:
        """
        mkdir -p {output}
        if [[ -s {input.ifile}]]
        then
            echo "{input.ifile} is empty"
        else
            wally region -R {input.ifile} -x {params.x} -y {params.y} -b {params.genebed} -cup -g {input.ref} {input.bam}
        fi
        """

rule copy_plots:
    input:
        snapshots = 'final/plots/{sample}',
        refinement = "linkage/{sample}/{sample}.refinement.tab"
    log: config['logdir'] + "/{sample}/copy_plots.log"
    params:
        prefix = "linkage/{sample}/{sample}",
        destination = "final/plots/{sample}"
    output:
        temp(touch('final/{sample}.plotscopied'))
    resources:
        mem_mb=500,
        runtime="1h",
        threads=1, 
        outstr=lambda wildcards: f'logs/copy_plots/{wildcards.sample}'
    shell:
        """
        mv {params.prefix}*.png {params.destination}/
        """

rule plots_final:
    input:
        plots = expand("final/{sample}.plotscopied", sample = SAMPLES)
    output:
        touch('final/PLOTSCOPIED')

# TODO: Create a rule that checks for alignment outside of the main construct sequence as a measure of contamination
# TODO: Generate an HTML report per sample with graphics to show linkage information and sample feasibility
