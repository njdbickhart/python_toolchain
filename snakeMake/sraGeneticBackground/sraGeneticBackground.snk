import os
import subprocess as sp

shell.executable("/usr/bin/bash")

configfile : "default.json"
WKDIR = os.getcwd()

os.makedirs("fastas", exist_ok=True)
os.makedirs("logs", exist_ok=True)
os.makedirs("TEMP", exist_ok=True)
os.makedirs("final", exist_ok=True)

def getAssemblyName(v):
    return os.path.basename(v).replace('.fa', '')

SAMPLES = set([v[1] for v in config["samples"].values()])
PAIRED_SRA = [v[0] for v in config["samples"].items() if v[1][0] == "P"]
SINGLE_SRA = [v[0] for v in config["samples"].items() if v[1][0] == "S"]
SRA_TOTAL = [v[0] for v in config["samples"].items()]
CALLERS = ["freebayes", "gatk"]

localrules: all, finished_download, temp_aggregate, finished_aggregate, consolidate_stats, plot_stats

wildcard_constraints:
    paired = "|".join(SRA_TOTAL),
    single = "|".join(SRA_TOTAL),
    callers = "|".join(CALLERS)

onerror:
    print("Error! Mailing log...")
    shell("tail -n 100 {log} | mail -s 'sraGeneticBackgroundError' derek.bickhart@hendrix-genetics.com")
    print("Done")

rule all:
    input:
        expand("calls/{callers}/{samples}.gvcf.gz",
            callers=CALLERS, samples = SAMPLES),
        "final/combined_bam_stats.tab",
        "final/combined_bam_stats.pdf",
        'FinishedAggregate',
        'FinishedDownload'

rule freebayes_only:
    input:
        expand("calls/freebayes/{samples}.gvcf.gz", samples = SAMPLES),
        "final/combined_bam_stats.tab",
        "final/combined_bam_stats.pdf",
        'FinishedAggregate',
        'FinishedDownload'

rule download_paired:
    #input:
    #    lambda wildcards: config["samples"][wildcards.srafiles][0]
    output:
        temp("fastas/{paired}_1.fastq"),
        temp("fastas/{paired}_2.fastq")
    params:
        download_folder = "fastas",
        mem="9000",
        time="1-0"
    threads: 3
    conda:
        "envs/base.yaml"
    log:
        "logs/{paired}.download.log"
    conda:
        "envs/sra.yaml"
    shell:
        "fasterq-dump {wildcards.paired} --split-files -O {params.download_folder} 2> {log}"

rule download_single:
    output:
        temp("fastas/{single}.fastq")
    params:
        download_folder = "fastas",
        mem="6000",
        time="1-0"
    threads: 3
    conda:
        "envs/base.yaml"
    log:
        "logs/{single}.download.log"
    conda:
        "envs/sra.yaml"
    shell:
        "fasterq-dump {wildcards.single} -O {params.download_folder} 2> {log}"

rule bwa_index:
    input:
        config["assembly"]
    output:
        temp(touch("Indexed"))
    log:
        "logs/assembly_indexing.log"
    conda:
        "envs/base.yaml"
    params:
        mem = "8000",
        time = "1-0"
    threads: 1
    shell:
        """
        bwa index {input} 2> {log}
        samtools faidx {input} 2> {log}
        picard CreateSequenceDictionary R={input}
        """

def getAllFQs(wildcards):
    files = []
    if len(PAIRED_SRA) > 0:
        files.extend(expand("fastas/{paired}_{ext}.fastq", paired=PAIRED_SRA, ext=[1,2]))
    if len(SINGLE_SRA) > 0:
        files.extend(expand("fastas/{single}.fastq", single = SINGLE_SRA))
    return files

rule finished_download:
    input:
        "Indexed",
        getAllFQs
    output:
        temp(touch('FinishedDownload'))

# Trick rule to create empty files for snakemake organization
rule temp_aggregate:
    input:
        finished = 'FinishedDownload'
    output:
        expand("mapped/{samples}", samples=SAMPLES)
    params:
        samples = SAMPLES
    run:
        os.makedirs("mapped", exist_ok=True)
        for i in params["samples"]:
            cmd = ['touch', 'mapped/' + i]
            print(cmd)
            sp.call(cmd)

rule finished_aggregate:
    input:
        fai = "Indexed",
        fqs = expand("fastas/{paired}_{ext}.fastq", paired=PAIRED_SRA, ext=[1,2]),
        sqs = expand("fastas/{single}.fastq", single = SINGLE_SRA),
        bams = expand("mapped/{samples}.merged.bam", samples=SAMPLES)
    output:
        temp(touch('FinishedAggregate'))

rule aggregate_reads:
    input:
        sample = ancient("mapped/{samples}"),
        reference = config["assembly"]
    output:
        temp("mapped/{samples}.merged.bam"),
        temp("mapped/{samples}.merged.bam.bai")
    log:
        "logs/{samples}_aggregate.log"
    threads: 8
    conda:
        "envs/base.yaml"
    params:
        mem = "30000",
        sname = "{samples}",
        config = "default.json",
        fastdir = "fastas",
        time="2-0"
    shell:
        """
        python3 {workflow.basedir}/scripts/aggregateSampleBams.py {params.config} {params.sname} {params.fastdir} {input.reference} {log}
        """

rule get_bam_stats:
    input:
        bam = ancient("mapped/{samples}.merged.bam"),
        bai = ancient("mapped/{samples}.merged.bam.bai"),
        reference = config["assembly"],
        fai = "Indexed",
    output:
        stats = temp("final/{samples}.stats.tab")
    log:
        "logs/{samples}_stats.log"
    threads: 2
    params:
        mem = "9000",
        sname = "{samples}",
        threshold = 4,
        time="2:00:00"
    shell:
        """
        python3 {workflow.basedir}/scripts/calc_bam_stats.py {input.bam} {params.threshold} {params.sname} {output.stats}
        """

rule consolidate_stats:
    input:
        expand("final/{samples}.stats.tab", samples=SAMPLES)
    output:
        "final/combined_bam_stats.tab"
    run:
        with open(output[0], 'w') as outfile:
            outfile.write("SNAME\tThreshBp\tZbp\tMean\tQ25\tMedian\tQ75\tMax\tStdev\tsub15\tsub30\tgt30\n")
            for i in input:
                with open(i, 'r') as infile:
                    for l in infile:
                        outfile.write(l)

rule plot_stats:
    input:
        "final/combined_bam_stats.tab"
    output:
        "final/combined_bam_stats.pdf"
    shell:
        """
        python3 {workflow.basedir}/scripts/plot_bam_stats.py {input} {output}
        """

rule strelka:
    input:
        bams = expand("mapped/{samples}.merged.bam", samples=SAMPLES),
        reference = config["assembly"]
    output:
        "calls/strelka/variants.vcf.gz"
    log:
        "logs/strelka.log"
    threads: 20
    conda:
        "envs/strelka.yaml"
    params:
        mem= "50000",
        concatfiles = lambda wildcards, input: " ".join([f'--bam {x}' for x in input.bams]),
        outputfolder = "calls/strelka",
        time="1-0"
    shell:
        """
        configureStrelkaGermlineWorkflow.py {params.concatfiles} --referenceFasta {input.reference} --runDir {params.outputfolder}

        {params.outputfolder}/runWorkflow.py -m local -j 20
        """

rule freebayes:
    input:
        ref= config["assembly"],
        samples= ["mapped/{samples}.merged.bam"],
        bai = "mapped/{samples}.merged.bam.bai"
    output:
        "calls/freebayes/{samples}.gvcf"
    log:
        "logs/freebayes.{samples}.log"
    conda:
        "envs/freebayes.yaml"
    params:
        extra="--min-repeat-entropy 1 --gvcf",
        chunksize=1000000,
        mem="15000",
        time="2-0"
    threads: 10
    script:
        "scripts/freebayes_lift.py"

rule haplotype_caller:
    input:
        # single or list of bam files
        bam="mapped/{samples}.merged.bam",
        bai = "mapped/{samples}.merged.bam.bai",
        ref=config["assembly"]
    output:
        gvcf = "calls/gatk/{samples}.gvcf"
    log:
        "logs/gatk/{samples}.log",
    params:
        extra=f'{WKDIR}/TEMP',  # optional
        java_opts="-j Xmx25G -j XX:+UseParallelGC -j XX:ParallelGCThreads=4",  # optional
        mem="25000",
        time="4-0",
        outputbase="calls/gatk/{samples}"
    threads: 21
    resources:
        mem_mb=4096
    shell:
        """
        python3 {workflow.basedir}/scripts/haplotypecaller.py -b {input.bam} -F {input.ref} -e {params.extra} {params.java_opts} -o {params.outputbase} -v {output.gvcf} -t {threads} 2> {log}
        """

rule gzip_gvcfs:
    input:
        "calls/{callers}/{samples}.gvcf"
    output:
        protected("calls/{callers}/{samples}.gvcf.gz")
    threads: 1
    params:
        mem="1000",
        time="2:00:00"
    shell:
        """
        bgzip {input}
        """
